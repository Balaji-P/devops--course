<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>The DevOps Ideal | vfarcic.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="The DevOps Ideal" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/devops20/devops-ideal.html" />
<meta property="og:url" content="http://localhost:4000/devops20/devops-ideal.html" />
<meta property="og:site_name" content="vfarcic.github.io" />
<script type="application/ld+json">
{"url":"http://localhost:4000/devops20/devops-ideal.html","@type":"WebPage","headline":"The DevOps Ideal","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=d7b0fccce7a840960ab2c608e690e524c171b2d9">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://localhost:4000/">vfarcic.github.io</a></h1>
      

      <h1 id="the-devops-ideal">The DevOps Ideal</h1>

<p>Working on small greenfield projects is great. The last one I was involved with was during the summer of 2015 and, even though it had its share of problems, it was a real pleasure. Working with a small and relatively new set of products allowed us to choose technologies, practices, and frameworks we liked. Shall we use microservices? Yes, why not. Shall we try Polymer and GoLang? Sure! Not having baggage that holds you down is a wonderful feeling. A wrong decision would put us back for a week, but it would not put in danger years of work someone else did before us. Simply put, there was no legacy system to think about and be afraid of.</p>

<p>Most of my career was not like that. I had the opportunity, or a curse, to work on big inherited systems. I worked for companies that existed long before I joined them and, for better or worse, already had their systems in place. I had to balance the need for innovation and improvement with obvious requirement that existing business must continue operating uninterrupted. During all those years I was continuously trying to discover new ways to improve those systems. It pains me to admit, but many of those attempts were failures.</p>

<p>We’ll explore those failures in order to understand better the motivations that lead to the advancements we’ll discuss throughout this books.</p>

<h2 id="continuous-integration-delivery-and-deployment">Continuous Integration, Delivery, and Deployment</h2>

<p>Discovering CI and, later on, CD, was one of the crucial points in my career. It all made perfect sense. The integration phase back in those days could last anything from days to weeks or even months. It was the period we all dreaded. After months of work performed by different teams working on different services or applications, the first day of the integration phase was the definition of hell on earth. If I didn’t know better, I’d say that Dante was a developer and wrote Infierno during the integration phase.</p>

<p>On the dreaded first day of the integration phase, we would all come to the office with grim faces. Only whispers could be heard while the integration engineer would announce that the whole system was set up, and the “game” could begin. He would turn it on and, sometimes, the result would be a blank screen. Months of work in isolation would prove, one more time, to be a disaster. Services and applications could not be integrated, and the long process of fixing problems would begin. In some cases, we would need to redo weeks of work. Requirements defined in advance were, as always, subject to different interpretations and those differences are nowhere more noticeable than in the integration phase.</p>

<p>Then <em>eXtreme Programming (XP)</em> practices came into existence and, with them, <em>continuous integration (CI)</em>. The idea that integration should be done continuously today sounds like something obvious. Duh! Of course, you should not wait until the last moment to integrate! Back then, in the waterfall era, such a thing was not so obvious as today. We implemented a continuous integration pipeline and started checking out every commit, running static analysis, unit and functional tests, packaging, deploying and running integration tests. If any of those phases failed, we would abandon what we were doing and made fixing the problem detected by the pipeline our priority. The pipeline itself was fast. Minutes after someone would make a commit to the repository we would get a notification if something failed. Later on, <em>continuous delivery (CD)</em> started to take ground, and we would have confidence that every commit that passed the whole pipeline could be deployed to production. We could do even better and not only attest that each build is production ready, but apply <em>continuous deployment</em> and deploy every build without waiting for (manual) confirmation from anyone. And the best part of all that was that everything was fully automated.</p>

<p>It was a dream come true. Literally! It was a dream. It wasn’t something we managed to turn into reality. Why was that? We made mistakes. We thought that CI/CD is a task for the operations department (today we’d call them <em>DevOPS</em>). We thought that we could create a process that wraps around applications and services. We thought that CI tools and frameworks are ready. We thought that architecture, testing, business negotiations and other tasks were the job for someone else. We were wrong. I was wrong.</p>

<p>Today I know that successful CI/CD means that no stone can be left unturned. We need to influence everything; from architecture through testing, development and operations all the way until management and business expectations. But let us go back again. What went wrong in those failures of mine?</p>

<h3 id="architecture">Architecture</h3>

<p>Trying to fit a monolithic application developed by many people throughout the years, without tests, with tight coupling and outdated technology is like an attempt to make an eighty-year-old lady look young again. We can improve her looks, but the best we can do is make her look a bit less old, not young. Some systems are, simply put, too old to be worth the “modernization” effort. I tried it, many times, and the result was never as expected. Sometimes, the effort in making it “young again” is not cost effective. On the other hand, I could not go to the client of, let’s say, a bank, and say “we’re going to rewrite your whole system.” Risks are too big to rewrite everything and, be it as it might, due to its tight coupling, age, and outdated technology, changing parts of it is not worth the effort. The commonly taken option was to start building the new system and, in parallel, maintain the old one until everything was done. That was always a disaster. It can take years to finish such a project, and we all know what happens with things planned for such a long term. That’s not even the waterfall approach. That’s like standing at the bottom of Niagara Falls wondering why you get wet. Even doing trivial things like updating the JDK was quite a feat. And those were the cases when I would consider myself lucky. What would you do with, for example, codebase done in Fortran or Cobol?</p>

<p>Then I heard about microservices. It was like music to my ears. The idea that we can build many small independent services that can be maintained by small teams, have codebase that can be understood in no time, being able to change framework, programming language or a database without affecting the rest of the system and being able to deploy it independently from the rest of the system was too good to be true. We could, finally, start taking parts of the monolithic application out without putting the whole system at (significant) risk. It sounded too good to be true. And it was. Benefits came with downsides. Deploying and maintaining a vast number of services turned out to be a heavy burden. We had to compromise and start standardizing services (killing innovation), we created shared libraries (coupling again), we were deploying them in groups (slowing everything), and so on. In other words, we had to remove the benefits microservices were supposed to bring. And let’s not even speak of configurations and the mess they created inside servers. Those were the times I try to forget. We had enough problems like that with monoliths. Microservices only multiplied them. It was a failure. However, I was not yet ready to give up. Call me a masochist.</p>

<p>I had to face problems one at a time, and one of the crucial ones was deployments.</p>

<h3 id="deployments">Deployments</h3>

<p>You know the process. Assemble some artifacts (JAR, WAR, DLL, or whatever is the result of your programming language), deploy it to the server that is already polluted with… I cannot even finish the sentence because, in many cases, we did not even know what was on the servers. With enough time, any server maintained manually becomes full of “things”. Libraries, executables, configurations, gremlins and trolls. It would start to develop its own personality. Old and grumpy, fast but unreliable, demanding, and so on. The only thing all the servers had in common was that they were all different, and no one could be sure that software tested in, let’s say, pre-production environment would behave the same when deployed to production. It was a lottery. You might get lucky, but most likely you won’t. Hope dies last.</p>

<p>You might, rightfully, wonder why we didn’t use virtual machines in those days. Well, there are two answers to that question, and they depend on the definition of “those days”. One answer is that in “those days” we didn’t have virtual machines, or they were so new that management was too scared to approve their usage. The other answer is that later on we did use VMs, and that was the real improvement. We could copy production environment and use it as, let’s say testing environment. Except that there was still a lot of work to update configurations, networking, and so on. Besides, we still did not know what was accumulated on those machines throughout the years. We just knew how to duplicate them. That still did not solve the problem that configurations were different from one VM to another as well as that a copy is the same as the original only for a short period. Do the deployment, change some configuration, bada bing, bada boom, you go back to the problem of testing something that is not the same as it will be in production. Differences accumulate with time unless you have a repeatable and reliable automated process instead of manual human interventions. If such a thing would exist, we could create immutable servers. Instead of deploying applications to existing servers and go down the path of accumulating differences, we could create a new VM as part of the CI/CD pipeline. So, instead of creating JARs, WAR, DLLs, and so on, we started creating VMs. Every time there was a new release it would come as a complete server built from scratch. That way we would know that what was tested is what goes into production. Create new VM with software deployed, test it and switch your production router to point from the old to the new one. It was awesome, except that it was slow and resource demanding. Having a separate VM for each service is overkill. Still, armed with patience, immutable servers were a good idea, but the way we used that approach and the tools required to support it were not good enough.</p>

<h3 id="orchestration">Orchestration</h3>

<p>The orchestration was the key. <em>Puppet</em> and <em>Chef</em> proved to be a big help. Programming everything related to servers setup and deployment was a huge improvement. Not only that the time needed to setup servers and deploy software dropped drastically, but we could, finally, accomplish a more reliable process. Having humans (read operations department) manually running those types of tasks was a recipe for disaster. Finally a story with a happy ending? Not really. You probably started noticing a pattern. As soon as one improvement was accomplished, it turned out that it, often, comes with a high price. Given enough time, Puppet and Chef scripts and configurations turn into an enormous pile of <em>**</em> (I was told not to use certain words so please fill in the blanks with your imagination). Maintaining them tends to become a nightmare in itself. Still, with orchestration tools, we could drastically reduce the time it took to create immutable VMs. Something is better than nothing.</p>

<h2 id="the-light-at-the-end-of-the-deployment-pipeline">The Light at the End of the Deployment Pipeline</h2>

<p>I could go on and on describing problems we faced. Don’t take me wrong. All those initiatives were improvements and have their place in software history. But history is the past, and we live in the present trying to look into the future. Many, if not all of the problems we had before are now solved. <em>Ansible</em> proved that orchestration does not need to be complicated to set up nor hard to maintain. With the appearance of <em>Docker</em>, containers are slowly replacing VMs as the preferable way to create immutable deployments. New operating systems are emerging and fully embracing containers as first class citizens. Tools for service discovery are showing us new horizons. <em>Swarm, Kubernetes and Mesos/DCOS</em> are opening doors into areas that were hard to imagine only a few years ago.</p>

<p>Microservices are slowly becoming the preferred way to build big, easy to maintain and highly scalable systems thanks to tools like <em>Docker, CoreOS, etcd, Consul, Fleet, Mesos, Rocket</em>, and others. The idea was always great, but we did not have the tools to make it work properly. Now we do! That does not mean that all our problems are gone. It means that when we solve one problem, the bar moves higher up, and new issues emerge.</p>

<p>I started by complaining about the past. That will not happen again. This book is for readers who do not want to live in the past but present. This book is about preparations for the future. This book is about stepping through the looking glass, about venturing into new areas and about looking at things from a new angle.</p>

<blockquote>
  <p><em>This is your last chance. After this, there is no turning back. You take the blue pill - the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill - you stay in Wonderland and I show you how deep the rabbit-hole goes.</em></p>

  <p>– Morpheus (Matrix)</p>
</blockquote>

<p>If you took the blue pill, I hope that you didn’t buy this book and got this far by reading the free sample. There are no hard feelings. We all have different aspirations and goals. If, on the other hand, you chose the red one, you are in for a ride. It will be like a roller coaster, and we are yet to discover what awaits us at the end of the ride.</p>

<h1 id="the-devops-20-toolkit">The DevOps 2.0 Toolkit</h1>

<p>This article is the first chapter of <a href="http://www.amazon.com/dp/B01BJ4V66M">The DevOps 2.0 Toolkit: Automating the Continuous Deployment Pipeline with Containerized Microservices</a> book.</p>

<p>The book is about different techniques that help us architect software in a better and more efficient way with <em>microservices</em> packed as <em>immutable containers</em>, <em>tested</em> and <em>deployed continuously</em> to servers that are <em>automatically provisioned</em> with <em>configuration management</em> tools. It’s about fast, reliable and continuous deployments with <em>zero-downtime</em> and ability to <em>roll-back</em>. It’s about <em>scaling</em> to any number of servers, the design of <em>self-healing systems</em> capable of recuperation from both hardware and software failures and about <em>centralized logging and monitoring</em> of the cluster.</p>

<p>In other words, this book envelops the full <em>microservices development and deployment lifecycle</em> using some of the latest and greatest practices and tools. We’ll use <em>Docker, Ansible, Ubuntu, Docker Swarm and Docker Compose, Consul, etcd, Registrator, confd, Jenkins, nginx</em>, and so on. We’ll go through many practices and, even more, tools.</p>

<p>The book is available from Amazon (<a href="http://www.amazon.com/dp/B01BJ4V66M">Amazon.com</a> and other worldwide sites) and <a href="https://leanpub.com/the-devops-2-toolkit">LeanPub</a>.</p>


      
      <div class="footer border-top border-gray-light mt-5 pt-3 text-right text-gray">
        This site is open source. <a href="https://github.com/vfarcic/vfarcic.github.io/edit/gh-pages/devops20/devops-ideal.md">Improve this page</a>.
      </div>
      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
