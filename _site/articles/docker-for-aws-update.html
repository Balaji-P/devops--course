<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Extending The Capacity Of A Docker For AWS Cluster | vfarcic.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Extending The Capacity Of A Docker For AWS Cluster" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/articles/docker-for-aws-update.html" />
<meta property="og:url" content="http://localhost:4000/articles/docker-for-aws-update.html" />
<meta property="og:site_name" content="vfarcic.github.io" />
<script type="application/ld+json">
{"url":"http://localhost:4000/articles/docker-for-aws-update.html","@type":"WebPage","headline":"Extending The Capacity Of A Docker For AWS Cluster","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=d7b0fccce7a840960ab2c608e690e524c171b2d9">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://localhost:4000/">vfarcic.github.io</a></h1>
      

      <h1 id="extending-the-capacity-of-a-docker-for-aws-cluster">Extending The Capacity Of A Docker For AWS Cluster</h1>

<p>One of the big benefits <a href="https://store.docker.com/editions/community/docker-ce-aws">Docker For AWS</a> provides is the ability to perform rolling updates to infrastructure services. Almost any aspect of the cluster can be changed without downtime. Moreover, most of the infrastructure is immutable. For example, if we choose to upgrade Docker Server running on the nodes, <em>Docker For AWS</em> will destroy the servers and put new ones based on a different image.</p>

<p>There are many other advantages behind <em>Docker For AWS</em>. However, the objective of this article is to show how we can extend the capacity of a cluster and to explain what’s happening in background.</p>

<p>I’ll assume that you already have a <em>Docker For AWS</em> cluster. If you don’t, please follow the instructions from <a href="https://youtu.be/uWTL8gwzZz4">Docker for AWS</a> video.</p>

<p>We’ll use CloudFormation UI to update the cluster. I think that’s a bad practice and you should use AWS CLI to accomplish the same result. However, since it is often easier to understand the process through UIs, we’ll stick with it (for now).</p>

<h2 id="extending-the-capacity-through-cloudformation-ui">Extending The Capacity Through CloudFormation UI</h2>

<p>Among other resources, <em>Docker For AWS</em> template created two auto-scaling groups. One is used for masters and the other for workers. Those security groups have multiple purposes.</p>

<p>If we choose to update the stack to, for example, change the size of the nodes or upgrade Docker server to a newer version, the template will temporarily increase the number of nodes by one and shut down one of the old ones. The replicas that were running on the old server will be moved to the new one. Once the new server is created, it will move to the next, and the next after that, all the way until all the nodes are replaced. The process is very similar to rolling updates we performed by Swarm when updating services. The same process is done whenever we decide to update any aspect of the <em>Docker For AWS</em> stack.</p>

<p>Similarly, if one of the nodes fail health checks, the stemplate will increase auto-scaling group by one so that a new node is created in its place and, once everything goes back to “normal” update the ASG back to its initial value.</p>

<p>In all those cases, not only that new nodes will be created through auto-scaling groups, but they will also join the cluster as a manager or a worker depending on the type of the server that is being replaced.</p>

<p>We will explore failure recovery in the chapter dedicated to self-healing applied to infrastructure. For now, we’ll limit the scope to an example how to update the CloudFormation stack that created our cluster. We even have a a perfect use-case. Our ElasticSearch service needs a worker node, and it needs it to be bigger than those we use as managers. Let’s create it.</p>

<p>We’ll start by opening CloudFormation home screen.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>open <span class="s2">"https://us-east-2.console.aws.amazon.com/cloudformation/home"</span>
</code></pre></div></div>

<p>Please select the stack. Click the <em>Actions</em> drop-down list and select the <em>Update Stack</em> item. Click the <em>Next</em> button</p>

<p>You will be presented with the same initial screen you saw while we were creating the <em>Docker For AWS</em> stack. The only difference is that the values are now populated with choices we made previously.</p>

<p>We can change anything we want. Not only that the changes will be applied accordingly, but the process will use rolling updates to avoid downtime. Whether you will have downtime or not depends on the capabilities of your services. If needed, the process will change one node at the time. If you’re running multiple replicas of a service, the worst case scenario is that you will experience degraded performance for a short period. However, services that are not scalable like, for example, Prometheus, will experience downtime.</p>

<p>When a node is destroyed, Swarm will move it to a newly created server. If the state of that service is on a network drive like EFS, it will continue working as if nothing happened. However, we must count the time between the service failure due to the destruction of the node and until it is up and running again. In most cases that should be only a couple of seconds. No matter how short the downtime is, it is still a period during which our non-scalable services are not operational. Be it as it may, not all services are scalable, and the process is the best we can do. If there is downtime, let it be as short as possible.</p>

<p>In this case, we won’t make an update that will force the system to recreate nodes. Instead, we’ll only add a new worker node.</p>

<p>Please scroll to the <em>Number of Swarm worker nodes?</em> field and change the value from <em>0</em> to <em>1</em>.</p>

<p>Since we defined that ElasticSearch should reserve 3GB of memory, we should change worker instance type. Our managers are using <em>t2.small</em> that comes with 2GB. The smallest instance that fulfills our requirements is <em>t2.medium</em> that comes 4GB of allocated memory.</p>

<p>Please change the value of the <em>Agent worker instance type?</em> drop-down list to <em>t2.medium</em>.</p>

<p>We will not change any other aspect of the cluster, so all that’s left is to click the <em>Next</em> button twice, and select the <em>I acknowledge that AWS CloudFormation might create IAM resources.</em> checkbox.</p>

<p>After a few moments, the <em>Preview your changes</em> section of the screen will be populated with the list of changes that will be applied to the cluster. Since this is a simple and non-destructive update, only a few resources related to auto-scaling groups will be updated.</p>

<p><img src="images/ch13/docker-for-aws-preview-changes.png" alt="Figure 13-4: Preview your changes screen from the Docker For AWS template" /></p>

<p>Click the <em>Update</em> button and relax. It’ll take a minute or two until the new server is created and it joins the cluster.</p>

<p>While waiting, we should explore a different method to accomplish the same result.</p>

<p>Please open the <em>Auto-Scaling Groups Details</em> screen.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>open <span class="s2">"https://console.aws.amazon.com/ec2/autoscaling/home?#AutoScalingGroups:view=details"</span>
</code></pre></div></div>

<p>You’ll be presented with the <em>Welcome to Auto Scaling</em> screen. Click the <em>Auto Scaling Groups: 2</em> link.</p>

<p>Select the item with the name starting with <em>[THE_NAME_OF_THE_STACK]-NodeAsg</em>. If, for example, your stack is called <em>my-stack</em>, the security group name should start with <em>my-stack-NodeAsg</em>. Click the <em>Actions</em> drop-down list, and select the <em>Edit</em> item. We’re looking for the <em>Desired</em> field located in the <em>details</em> tab. It can be changed to any value, and the number of workers would increase (or decrease) accordingly. We could do the same with the auto-scaling group associated with manager nodes. Do not make any change. We’re almost finished with this chapter, and we already have more than enough nodes for the services we’re running.</p>

<p>The knowledge that we can change the number of manager or worker nodes by changing the values in auto-scaling groups is essential. Later on, we’ll combine that with AWS API and Prometheus alerts to automate the process when certain conditions are met.</p>

<p>The new worker node should be up-and-running by now unless you are a very fast reader. If that’s the case, go and grab a coffee.</p>

<p>Let’s go back to the cluster and list the available nodes.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-i</span> <span class="o">[</span>PATH_TO_SSH_KEY] docker@<span class="nv">$CLUSTER_IP</span>

docker node <span class="nb">ls</span>
</code></pre></div></div>

<p>Please replace <code class="highlighter-rouge">[PATH_TO_SSH_KEY]</code> with the path to your key (e.g. <code class="highlighter-rouge">workshop.pem</code>).</p>

<p>The output is as follows (IDs are removed for brevity).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HOSTNAME                                    STATUS AVAILABILITY MANAGER STATUS
ip-172-31-2-119.us-east-2.compute.internal  Ready  Active
ip-172-31-32-225.us-east-2.compute.internal Ready  Active       Leader
ip-172-31-10-207.us-east-2.compute.internal Ready  Active       Reachable
ip-172-31-30-18.us-east-2.compute.internal  Ready  Active       Reachable
</code></pre></div></div>

<p>In your case, the list of the nodes might be different. My cluster had only three managers before the update and now a new node is added to the mix. Since its a worker, manager status is empty.</p>

<p>Your first thought might be that it is a simple process. After all, all that AWS did was create a new VM. That is right from AWS point of view, but there are a few other things that happened in the background.</p>

<p>During VM initialization, it contacted Dynamo DB to find out the address of the primary manager and the access token. Equipped with that info, it sent a request to that manager to join the cluster. From there on, the new node (in this case worked) is available as part of the Swarm cluster.</p>


      
      <div class="footer border-top border-gray-light mt-5 pt-3 text-right text-gray">
        This site is open source. <a href="https://github.com/vfarcic/vfarcic.github.io/edit/gh-pages/articles/docker-for-aws-update.md">Improve this page</a>.
      </div>
      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
