<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Docker Swarm Introduction (Tour Around Docker 1.12 Series) | vfarcic.github.io</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Docker Swarm Introduction (Tour Around Docker 1.12 Series)" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/docker-swarm/docker-swarm.html" />
<meta property="og:url" content="http://localhost:4000/docker-swarm/docker-swarm.html" />
<meta property="og:site_name" content="vfarcic.github.io" />
<script type="application/ld+json">
{"url":"http://localhost:4000/docker-swarm/docker-swarm.html","@type":"WebPage","headline":"Docker Swarm Introduction (Tour Around Docker 1.12 Series)","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=d7b0fccce7a840960ab2c608e690e524c171b2d9">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://localhost:4000/">vfarcic.github.io</a></h1>
      

      <h1 id="docker-swarm-introduction-tour-around-docker-112-series">Docker Swarm Introduction (Tour Around Docker 1.12 Series)</h1>

<p><img src="/img/swarm/swarm.png" alt="Docker Swarm" /></p>

<p>Docker just published a new Docker Engine v1.12. It is the most significant release since v1.9. Back then, we got Docker networking that, finally, made containers ready for use in clusters. With v1.12, Docker is reinventing itself with a whole new approach to cluster orchestration. Say goodbye to Swarm as a separate container that depends on an external data registry and welcome the <strong>new Docker Swarm</strong>. Everything you’ll need to manage your cluster is now incorporated into Docker Engine. Swarm is there. Service discovery is there. Improved networking is there.</p>

<p>The old Swarm (before Docker v1.12) used <strong>fire-and-forget principle</strong>. We would send a command to Swarm master, and it would execute that command. For example, if we would send it something like <code class="highlighter-rouge">docker-compose scale go-demo=5</code>, the old Swarm would evaluate the current state of the cluster, discover that, for example, only one instance is currently running, and decide that it should run four more. Once such a decision is made, the old Swarm would send commands to Docker Engines. As a result, we would have five containers running inside the cluster. For all that to work, we were required to set up Swarm agents (as separate containers) on all the nodes that form the cluster and hook them into one of the supported data registries (Consul, etcd, and Zookeeper). The problem was that Swarm was executing commands we send it. It was not maintaining the desired state. We were, effectively, telling it what we want to happen (e.g. scale up), not the state we desired (make sure that five instances are running). Later on, the old Swarm got the feature that would reschedule containers from failed nodes. However, that feature had a few problems that prevented it from being a reliable solution (e.g. failed containers were not removed from the overlay network).</p>

<p>Now we got a brand new Swarm. It is part of Docker Engine (no need to run it as separate containers), it has incorporated service discovery (no need to set up Consul or whatever is your data registry of choice), it is designed from the ground up to accept and maintain the desired state, and so on. It is a truly major change in how we deal with cluster orchestration.</p>

<p>In the past, I was inclined towards the old Swarm more than Kubernetes. However, that inclination was only slight. There were pros and cons for using either solution. Kubernetes had a few features Swarm was missing (e.g. the concept of the desired state), the old Swarm shined with its simplicity and low usage of resources. With the new Swarm (the one that comes with v1.12), I have no more doubts which one to use. <strong>The new Swarm is a better choice than Kubernetes</strong>. It is part of Docker Engine, so the whole setup is a single command that tells an engine to join the cluster. The new networking works like a charm. The bundle that can be used to define services can be created from Docker Compose files, so there is no need maintain two sets of configurations (Docker Compose for development and a different one for orchestration). Most importantly, the new Docker Swarm continues being simple to use. From the very beginning, Docker community pledged that they are committed to simplicity and, with this release, they, once again, proved that to be true.</p>

<p>And that’s not all. The new release comes with a lot of other features that are not directly related with Swarm. However, the exploration of those features would require much more than one article. Therefore, today I’ll focus on Swarm and leave the rest for one of the next articles.</p>

<p>Since I believe that code (or in this case commands), explain things better than words, we’ll start with a demo of some of the new features introduced in version 1.12. Specifically, we’ll explore the new command <em>service</em>.</p>

<h2 id="environment-setup">Environment Setup</h2>

<p>The examples that follow assume that you have <a href="https://www.docker.com/products/docker-machine">Docker Machine</a> version v0.8+ that includes <a href="https://www.docker.com/products/docker-engine">Docker Engine</a> v1.12+. The easiest way to get them is through <a href="https://www.docker.com/products/docker-toolbox">Docker Toolbox</a>.</p>

<blockquote>
  <p>If you are a Windows user, please run all the examples from <em>Git Bash</em> (installed through <em>Docker Toolbox</em>).</p>
</blockquote>

<p>We’ll start by creating three machines that will simulate a cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-machine create <span class="nt">-d</span> virtualbox node-1

docker-machine create <span class="nt">-d</span> virtualbox node-2

docker-machine create <span class="nt">-d</span> virtualbox node-3

docker-machine <span class="nb">ls</span>
</code></pre></div></div>

<p>The output of the <code class="highlighter-rouge">ls</code> command is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME     ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS
node-1   -        virtualbox   Running   tcp://192.168.99.100:2376           v1.12.0
node-2   -        virtualbox   Running   tcp://192.168.99.101:2376           v1.12.0
node-3   -        virtualbox   Running   tcp://192.168.99.102:2376           v1.12.0
</code></pre></div></div>

<p>Please note that Docker version <strong>MUST</strong> be 1.12 or higher. If it isn’t, please update your Docker Machine version, destroy the VMs, and start over.</p>

<p><img src="/img/swarm/nodes.png" alt="Machines running Docker Engines" /></p>

<p>With the machines up and running we can proceed and set up the Swarm cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-1<span class="k">)</span>

docker swarm init <span class="se">\</span>
    <span class="nt">--advertise-addr</span> <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span> <span class="se">\</span>
    <span class="nt">--listen-addr</span> <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span>:2377
</code></pre></div></div>

<p>The first command set environment variables so that local Docker Engine is pointing to the <em>node-1</em>. The second initialized Swarm on that machine. Right now, our Swarm cluster consists of only one VM.</p>

<p>Let’s add the other two nodes to the cluster.</p>

<p>As a way to increase security, a new node can be added to the cluster only if it contains the token generated when Swarm was initialized. The token was printed as a result of the <code class="highlighter-rouge">docker swarm init</code> commmand. You can copy and paste the code from the output or use the <code class="highlighter-rouge">join-token</code> command.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker swarm join-token <span class="nt">-q</span> worker
</code></pre></div></div>

<p>The output is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SWMTKN-1-24hd6kvr8ihzu7mtklhwj6p6hi1mv1uw6ohf2axtw9ada02hot-6ttad3td76xwvnctjnt3m0u41
</code></pre></div></div>

<p>Please note that this token was generated on my machine and, in your case, it will be different.</p>

<p>Let’s put the token into an environment variable and add the other two nodes as workers.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">TOKEN</span><span class="o">=</span><span class="k">$(</span>docker swarm join-token <span class="nt">-q</span> worker<span class="k">)</span>

<span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-2<span class="k">)</span>

docker swarm join <span class="nt">--token</span> <span class="nv">$TOKEN</span> <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span>:2377

<span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-3<span class="k">)</span>

docker swarm join <span class="nt">--token</span> <span class="nv">$TOKEN</span> <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span>:2377
</code></pre></div></div>

<p>The other two machines joined the cluster as workers. We can confirm that by sending the <code class="highlighter-rouge">node ls</code> command to the <em>Leader</em> node (<em>node-1</em>).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-1<span class="k">)</span>

docker node <span class="nb">ls</span>
</code></pre></div></div>

<p>The output of the <code class="highlighter-rouge">node ls</code> command is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
92ho364xtsdaq2u0189tna8oj *  node-1    Accepted    Ready   Active        Leader
c2tykql7a2zd8tj0b88geu45i    node-2    Accepted    Ready   Active
ejsjwyw5y92560179pk5drid4    node-3    Accepted    Ready   Active
</code></pre></div></div>

<p>The star tells us which node we are currently using. The <em>manager status</em> indicates that the <em>node-1</em> is the <em>leader</em>.</p>

<p><img src="/img/swarm/swarm-nodes.png" alt="Docker Swarm cluster with three nodes" /></p>

<p>In a production environment, we would probably set more than one node to be a manager and, thus, avoid deployment downtime if one of them fails. For the purpose of this demo, having one manager should suffice.</p>

<h2 id="deploying-container-to-the-cluster">Deploying Container To The Cluster</h2>

<p>Before we deploy a demo service, we should create a new network so that all containers that constitute the service can communicate with each other no matter on which nodes they are deployed.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network create <span class="nt">--driver</span> overlay go-demo
</code></pre></div></div>

<p>We can check the status of all networks with the command that follows.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network <span class="nb">ls</span>
</code></pre></div></div>

<p>The output of the <code class="highlighter-rouge">network ls</code> command is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NETWORK ID          NAME                DRIVER              SCOPE
e263fb34287a        bridge              bridge              local
c5b60cff0f83        docker_gwbridge     bridge              local
8d3gs95h5c5q        go-demo             overlay             swarm
4d0719f20d24        host                host                local
eafx9zd0czuu        ingress             overlay             swarm
81d392ce8717        none                null                local
</code></pre></div></div>

<p>As you can see, we have two networks that have the <em>swarm</em> scope. The one named <em>ingress</em> was created by default when we set up the cluster. The second (<em>go-demo</em>) was created with the <code class="highlighter-rouge">network create</code> command. We’ll assign all containers that constitute the <em>go-demo</em> service to that network.</p>

<p><img src="/img/swarm/swarm-nodes-sdn.png" alt="Docker Swarm cluster with Docker network (SDN)" /></p>

<p>The <em>go-demo</em> service requires two containers. Data will be stored in MongoDB. The back-end that uses that DB is defined as <em>vfarcic/go-demo</em> container.</p>

<p>Let’s start by deploying the <em>mongo</em> container somewhere within the cluster. Usually, we’d use constraints to specify the requirements for the container (e.g. HD type, the amount of memory and CPU, and so on). We’ll skip that and, simply, tell Swarm to deploy it anywhere within the cluster.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service create <span class="nt">--name</span> go-demo-db <span class="se">\</span>
  <span class="nt">--network</span> go-demo <span class="se">\</span>
  mongo
</code></pre></div></div>

<p>The <code class="highlighter-rouge">-p</code> argument sets the port to <em>27017</em>. Please note that, when it contains only a single value, the port will be reachable only through networks the container belongs to. In this case, we set the network to be <em>go-demo</em> (the one we created earlier). As you can see, the way we use <code class="highlighter-rouge">service create</code> is similar to the Docker <code class="highlighter-rouge">run</code> command you are, probably, already used to.</p>

<p>We can list all the running services.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service <span class="nb">ls</span>
</code></pre></div></div>

<p>Depending on how much time passed between <code class="highlighter-rouge">service create</code> and <code class="highlighter-rouge">service ls</code> commands, you’ll see the value of the <em>Replicas</em> column being zero or one. Immediately after creating the service, the value should be <em>0/1</em>, meaning that zero replicas are running and the objective is to have one. Once the <em>mongo</em> image is downloaded, and the container is running, the value should change to <em>1/1</em>.</p>

<p>The final output of the <code class="highlighter-rouge">service ls</code> command should be as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ID            NAME        REPLICAS  IMAGE  COMMAND
c8tjeq1hofjp  go-demo-db  1/1       mongo
</code></pre></div></div>

<p>If we need more information about the <em>go-demo-db</em> service, we can run the <code class="highlighter-rouge">service inspect</code> command.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service inspect go-demo-db
</code></pre></div></div>

<p>Now that the database is running, we can deploy the <em>go-demo</em> container.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service create <span class="nt">--name</span> go-demo <span class="se">\</span>
  <span class="nt">-p</span> 8080 <span class="se">\</span>
  <span class="nt">-e</span> <span class="nv">DB</span><span class="o">=</span>go-demo-db <span class="se">\</span>
  <span class="nt">--network</span> go-demo <span class="se">\</span>
  vfarcic/go-demo
</code></pre></div></div>

<p>There’s nothing new about that command. Internally, it exposes port 8080 and it belongs to the network <em>go-demo</em>. The environment variable <em>DB</em> is an internal requirement of the <em>go-demo</em> service that tells the code the address of the database.</p>

<p>At this point, we have two containers (<em>mongo</em> and <em>go-demo</em>) running inside the cluster and communicating with each other through the <em>go-demo</em> network. Please note that none of them is (yet) accessible from outside the network. At this point, your users do not have access to the service API. We’ll discuss this in more details in the next article. Until then, I’ll give you only a hint: <em>you need a proxy</em> capable of utilizing the new Swarm networking.</p>

<p><img src="/img/swarm/swarm-nodes-single-container.png" alt="Docker Swarm cluster containers communicating through the go-demo SDN" /></p>

<p>What happens if we want to scale one of the containers?</p>

<h2 id="scaling-services">Scaling Services</h2>

<p>We can, for example, tell Swarm that we want to run five replicas of the <em>go-demo</em> service.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service update <span class="nt">--replicas</span> 5 go-demo
</code></pre></div></div>

<p>With the <code class="highlighter-rouge">service update</code> command, we scheduled five replicas. After a short some time, Swarm will make sure that five instances of go-demo are running somewhere inside the cluster.</p>

<p>We can confirm that, indeed, five replicas are running through the, already familiar, <code class="highlighter-rouge">service ls</code> command.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service <span class="nb">ls</span>
</code></pre></div></div>

<p>The output is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ID            NAME        REPLICAS  IMAGE            COMMAND
1hzeaz2jxs5e  go-demo     5/5       vfarcic/go-demo
c8tjeq1hofjp  go-demo-db  1/1       mongo
</code></pre></div></div>

<p>As we can see, five out of five replicas of the <em>go-demo</em> container are running.</p>

<p>The <code class="highlighter-rouge">service ps</code> command provides more detailed information about a single service.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service ps go-demo
</code></pre></div></div>

<p>The output is as follows.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ID                         NAME       IMAGE            NODE    DESIRED STATE  CURRENT STATE               ERROR
40jasxwg45kg0p1ulht39904o  go-demo.1  vfarcic/go-demo  node-3  Running        Running about a minute ago
08mghas13b99e9bb4rsols77o  go-demo.2  vfarcic/go-demo  node-2  Running        Running 51 seconds ago
bus2hm8y6a113lnemrz5ke0yn  go-demo.3  vfarcic/go-demo  node-2  Running        Running 51 seconds ago
8vae12ugrrrakevey74p7hhsq  go-demo.4  vfarcic/go-demo  node-1  Running        Running 53 seconds ago
99uura8n1tgjtjk4tqp49mszz  go-demo.5  vfarcic/go-demo  node-3  Running        Running about a minute ago
</code></pre></div></div>

<p>We can see that the <em>go-demo</em> service is running five instances distributed across the three nodes. Since they all belong to the same <em>go-demo</em> network, they can communicate with each other no matter where they run inside the cluster. At the same time, none of them is accessible from “outside”.</p>

<p><img src="/img/swarm/swarm-nodes-replicas.png" alt="Docker Swarm cluster with go-demo service scaled to five replicas" /></p>

<p>What happens if one of the containers is stopped or if the entire node fails? After all, processes and nodes do fail sooner or later. Nothing is perfect, and we need to be prepared for such situations.</p>

<h2 id="failover">Failover</h2>

<p>Fortunately, failover strategies are part of Docker Swarm. Remember, when we execute a <code class="highlighter-rouge">service</code> command, we are not telling Swarm what to do but the state we desire. In turn, Swarm will do its best to maintain the specified state no matter what happens.</p>

<p>To test a failure scenario, we’ll destroy one of the nodes.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-machine rm <span class="nt">-f</span> node-3
</code></pre></div></div>

<p>Swarm needs a bit of time until it detects that the node is down. Once it does, it will reschedule containers. We can monitor the situation through the <code class="highlighter-rouge">service ps</code> command.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker service ps go-demo
</code></pre></div></div>

<p>The output (after rescheduling) is as follows.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ID                         NAME       SERVICE  IMAGE            LAST STATE              DESIRED STATE  NODE
czbpu8lrhds1wx0ml4vhv65s1  go-demo.1      vfarcic/go-demo  node-2  Running        Running 13 seconds ago
40jasxwg45kg0p1ulht39904o   <span class="se">\_</span> go-demo.1  vfarcic/go-demo  node-3  Shutdown       Running about a minute ago
08mghas13b99e9bb4rsols77o  go-demo.2      vfarcic/go-demo  node-2  Running        Running about a minute ago
bus2hm8y6a113lnemrz5ke0yn  go-demo.3      vfarcic/go-demo  node-2  Running        Running about a minute ago
8vae12ugrrrakevey74p7hhsq  go-demo.4      vfarcic/go-demo  node-1  Running        Running about a minute ago
cnbwfraw6jbkfzf9ufdv970bg  go-demo.5      vfarcic/go-demo  node-1  Running        Running 13 seconds ago
99uura8n1tgjtjk4tqp49mszz   <span class="se">\_</span> go-demo.5  vfarcic/go-demo  node-3  Shutdown       Running about a minute ago
</code></pre></div></div>

<p>As you can see, after a short period of time, Swarm rescheduled containers among healthy nodes (<em>node-1</em> and <em>node-2</em>) and changed the state of those that were running inside the failed node to <em>Shutdown</em>. If your output still shows that some instances are running on the <em>node-3</em>, please wait for a few moments and repeat the <code class="highlighter-rouge">service ps</code> command.</p>

<h2 id="what-now">What Now?</h2>

<p>That concludes the exploration of basic concepts of the new Swarm features we got with Docker v1.12.</p>

<p>Is this everything there is to know to run a Swarm cluster successfully? Not even close! What we explored by now is only the beginning. There are quite a few questions waiting to be answered. How do we expose our services to the public? How do we deploy new releases without downtime? Are there any additional tools we should use? I’ll try to give answers to those and quite a few other questions in future articles. The next one will be dedicated to the exploration of the ways we can expose our services to the public. We’ll try to integrate a proxy with a Swarm cluster.</p>


      
      <div class="footer border-top border-gray-light mt-5 pt-3 text-right text-gray">
        This site is open source. <a href="https://github.com/vfarcic/vfarcic.github.io/edit/gh-pages/docker-swarm/docker-swarm.md">Improve this page</a>.
      </div>
      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
